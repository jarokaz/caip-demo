{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on AI Platform with a custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment\n",
    "### Set a GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "ARTIFACT_STORE = 'gs://{}-artifact-store'.format(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_PATH = '{}/datasets/training.csv'.format(ARTIFACT_STORE)\n",
    "TESTING_DATA_PATH = '{}/datasets/testing.csv'.format(ARTIFACT_STORE)\n",
    "REGION = \"us-central1\"\n",
    "JOBDIR_BUCKET = '{}/jobs'.format(ARTIFACT_STORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training container image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_IMAGE_FOLDER = '../training_image'\n",
    "\n",
    "os.makedirs(TRAINING_IMAGE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../training_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINING_IMAGE_FOLDER/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN python -m pip install -U fire gcsfs\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../training_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINING_IMAGE_FOLDER/train.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import joblib\n",
    "import fire\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.manifold import TSNE \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def train(job_dir, data_path, n_features_options, l2_reg_options):\n",
    "    \n",
    "  # Load data from GCS\n",
    "  df_train = pd.read_csv(data_path)\n",
    "\n",
    "  y = df_train.octane\n",
    "  X = df_train.drop('octane', axis=1)\n",
    "    \n",
    "  # Configure a training pipeline\n",
    "  pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('regress', Ridge())\n",
    "  ])\n",
    "\n",
    "  # Configure a parameter grid\n",
    "  param_grid = [\n",
    "    {\n",
    "      'reduce_dim__n_components': n_features_options,\n",
    "      'regress__alpha': l2_reg_options\n",
    "    }\n",
    "  ]\n",
    "\n",
    "  # Tune hyperparameters\n",
    "  grid = GridSearchCV(pipeline, cv=10, n_jobs=None, param_grid=param_grid, scoring='neg_mean_squared_error', iid=False)\n",
    "  grid.fit(X, y)\n",
    "\n",
    "  logging.info(\"Best estimator: {}\".format(grid.best_params_))\n",
    "  logging.info(\"Best score: {}\".format(grid.best_score_))\n",
    "    \n",
    "  # Retrain the best model on a full dataset\n",
    "  best_estimator = grid.best_estimator_\n",
    "  trained_pipeline = best_estimator.fit(X, y)\n",
    "\n",
    "  # Save the model\n",
    "  model_filename = 'model.joblib'\n",
    "  joblib.dump(value=trained_pipeline, filename=model_filename)\n",
    "  gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "  subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "  logging.info(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  logging.basicConfig(level=logging.INFO)\n",
    "  fire.Fire(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.0 KiB before compression.\n",
      "Uploading tarball of [../training_image] to [gs://mlops-dev-100_cloudbuild/source/1583977371.65-0179be7af5384d5386aa88bd56c6e0a6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-dev-100/builds/24919490-a8fd-471b-8917-b51eb4cedf8f].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/24919490-a8fd-471b-8917-b51eb4cedf8f?project=286479790129].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"24919490-a8fd-471b-8917-b51eb4cedf8f\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-dev-100_cloudbuild/source/1583977371.65-0179be7af5384d5386aa88bd56c6e0a6.tgz#1583977371875128\n",
      "Copying gs://mlops-dev-100_cloudbuild/source/1583977371.65-0179be7af5384d5386aa88bd56c6e0a6.tgz#1583977371875128...\n",
      "/ [1 files][  1.1 KiB/  1.1 KiB]                                                \n",
      "Operation completed over 1 objects/1.1 KiB.                                      \n",
      "BUILD\n",
      "Pulling image: gcr.io/kaniko-project/executor:latest\n",
      "latest: Pulling from kaniko-project/executor\n",
      "916379538e2e: Pulling fs layer\n",
      "92cddcaa2f24: Pulling fs layer\n",
      "a0510fd8d15b: Pulling fs layer\n",
      "079ea9bc81db: Pulling fs layer\n",
      "e3a367cce5ee: Pulling fs layer\n",
      "0cae8e417729: Pulling fs layer\n",
      "8b200aba6e04: Pulling fs layer\n",
      "ae8dea9305e6: Pulling fs layer\n",
      "079ea9bc81db: Waiting\n",
      "e3a367cce5ee: Waiting\n",
      "0cae8e417729: Waiting\n",
      "8b200aba6e04: Waiting\n",
      "ae8dea9305e6: Waiting\n",
      "92cddcaa2f24: Verifying Checksum\n",
      "92cddcaa2f24: Download complete\n",
      "a0510fd8d15b: Verifying Checksum\n",
      "a0510fd8d15b: Download complete\n",
      "079ea9bc81db: Verifying Checksum\n",
      "079ea9bc81db: Download complete\n",
      "e3a367cce5ee: Verifying Checksum\n",
      "e3a367cce5ee: Download complete\n",
      "916379538e2e: Verifying Checksum\n",
      "916379538e2e: Download complete\n",
      "0cae8e417729: Verifying Checksum\n",
      "0cae8e417729: Download complete\n",
      "ae8dea9305e6: Verifying Checksum\n",
      "ae8dea9305e6: Download complete\n",
      "8b200aba6e04: Verifying Checksum\n",
      "8b200aba6e04: Download complete\n",
      "916379538e2e: Pull complete\n",
      "92cddcaa2f24: Pull complete\n",
      "a0510fd8d15b: Pull complete\n",
      "079ea9bc81db: Pull complete\n",
      "e3a367cce5ee: Pull complete\n",
      "0cae8e417729: Pull complete\n",
      "8b200aba6e04: Pull complete\n",
      "ae8dea9305e6: Pull complete\n",
      "Digest: sha256:2b54a743d46b5c4eff5772c68177958c6876bdf77b31bcda5a6af376b6c31428\n",
      "Status: Downloaded newer image for gcr.io/kaniko-project/executor:latest\n",
      "gcr.io/kaniko-project/executor:latest\n",
      "\u001b[36mINFO\u001b[0m[0000] Resolved base name gcr.io/deeplearning-platform-release/base-cpu to gcr.io/deeplearning-platform-release/base-cpu \n",
      "\u001b[36mINFO\u001b[0m[0000] Resolved base name gcr.io/deeplearning-platform-release/base-cpu to gcr.io/deeplearning-platform-release/base-cpu \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest gcr.io/deeplearning-platform-release/base-cpu \n",
      "\u001b[36mINFO\u001b[0m[0000] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest gcr.io/deeplearning-platform-release/base-cpu \n",
      "\u001b[36mINFO\u001b[0m[0000] Checking for cached layer gcr.io/mlops-dev-100/octane-regression-training/cache:0aba2d509a49e3dcd07594d0ac9b75dba08fb32e9be3f5f29932652501be2da1... \n",
      "\u001b[36mINFO\u001b[0m[0001] No cached layer found for cmd RUN python -m pip install -U fire gcsfs \n",
      "\u001b[36mINFO\u001b[0m[0001] Unpacking rootfs as cmd RUN python -m pip install -U fire gcsfs requires it. \n",
      "\u001b[36mINFO\u001b[0m[0137] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0143] Resolving paths                              \n",
      "\u001b[36mINFO\u001b[0m[0495] RUN python -m pip install -U fire gcsfs      \n",
      "\u001b[36mINFO\u001b[0m[0495] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0495] args: [-c python -m pip install -U fire gcsfs] \n",
      "Collecting fire\n",
      "  Downloading fire-0.2.1.tar.gz (76 kB)\n",
      "Requirement already up-to-date: gcsfs in /opt/conda/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.14.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib in /opt/conda/lib/python3.7/site-packages (from gcsfs) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.2 in /opt/conda/lib/python3.7/site-packages (from gcsfs) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /opt/conda/lib/python3.7/site-packages (from gcsfs) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: fsspec>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from gcsfs) (0.6.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from gcsfs) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib->gcsfs) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (45.2.0.post20200209)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Building wheels for collected packages: fire, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103528 sha256=eeb328c771bbcd816d927cc69d2944d597f9b3c7cfead43f07b52622ec4284e1\n",
      "  Stored in directory: /root/.cache/pip/wheels/a8/6d/a8/d81d42414b24203fc8beb0452deab949ba62fcfb8c7a49e4b6\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=25799525afe491f3f6682aef062757ebf515946477ff77242f0bd0f1eef15b22\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire termcolor\n",
      "Installing collected packages: termcolor, fire\n",
      "Successfully installed fire-0.2.1 termcolor-1.1.0\n",
      "\u001b[36mINFO\u001b[0m[0501] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0509] Resolving paths                              \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Your build timed out. Use the [--timeout=DURATION] flag to change the timeout threshold.\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) build 24919490-a8fd-471b-8917-b51eb4cedf8f completed with status \"TIMEOUT\"\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME=\"octane-regression-training\"\n",
    "IMAGE_TAG=\"latest\"\n",
    "IMAGE_URI=\"gcr.io/{}/{}:{}\".format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "\n",
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_IMAGE_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "N_FEATURES_OPTIONS=\"[2,4,6]\"\n",
    "L2_REG_OPTIONS=\"[0.1,0.2,0.3,0.5]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region $REGION \\\n",
    "--job-dir $JOBDIR_BUCKET/$JOB_NAME \\\n",
    "--master-image-uri $IMAGE_URI \\\n",
    "--scale-tier $SCALE_TIER \\\n",
    "-- \\\n",
    "--data_path $TRAINING_DATA_PATH  \\\n",
    "--n_features_options $N_FEATURES_OPTIONS \\\n",
    "--l2_reg_options $L2_REG_OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME\n",
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the trained model to model repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"octane-regressor-container\"\n",
    "\n",
    "!gsutil cp $JOBDIR_BUCKET/$JOB_NAME/model.joblib $ARTIFACT_BUCKET/models/$MODEL_NAME/model.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_PATH = '/tmp/model.joblib'\n",
    "\n",
    "!gsutil cp $ARTIFACT_BUCKET/models/$MODEL_NAME/model.joblib $LOCAL_PATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv(TESTING_DATA_PATH)\n",
    "predictor = joblib.load(LOCAL_PATH)\n",
    "\n",
    "y = df_test.octane\n",
    "X = df_test.drop('octane', axis=1)\n",
    "\n",
    "y_hat = predictor.predict(X)\n",
    "print(list(zip(y, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
