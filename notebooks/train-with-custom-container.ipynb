{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Cloud Build to use Kaniko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [builds/use_kaniko].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set builds/use_kaniko True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_FOLDER = '../training_image'\n",
    "os.makedirs(APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../training_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $APP_FOLDER/Dockerfile\n",
    "\n",
    "FROM gcr.io/jk-demo1/sklearn-cpu:latest\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../training_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $APP_FOLDER/train.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import joblib\n",
    "import fire\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.manifold import TSNE \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def train(job_dir, data_path, n_features_options, l2_reg_options):\n",
    "    \n",
    "  # Load data from GCS\n",
    "  df_train = pd.read_csv(data_path)\n",
    "\n",
    "  y = df_train.octane\n",
    "  X = df_train.drop('octane', axis=1)\n",
    "    \n",
    "  pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('regress', Ridge())\n",
    "  ])\n",
    "\n",
    "  param_grid = [\n",
    "    {\n",
    "      'reduce_dim__n_components': n_features_options,\n",
    "      'regress__alpha': l2_reg_options\n",
    "    }\n",
    "  ]\n",
    "\n",
    "  grid = GridSearchCV(pipeline, cv=10, n_jobs=None, param_grid=param_grid, scoring='neg_mean_squared_error', iid=False)\n",
    "  \n",
    "  grid.fit(X, y)\n",
    "\n",
    "  logging.info(\"Best estimator: {}\".format(grid.best_params_))\n",
    "  logging.info(\"Best score: {}\".format(grid.best_score_))\n",
    "    \n",
    "  # Retrain the best model on a full dataset\n",
    "  best_estimator = grid.best_estimator_\n",
    "  trained_pipeline = best_estimator.fit(X, y)\n",
    "\n",
    "  # Save the model\n",
    "  model_filename = 'model.joblib'\n",
    "  joblib.dump(value=trained_pipeline, filename=model_filename)\n",
    "  gcs_model_path = \"{}/trained_model/{}\".format(job_dir, model_filename)\n",
    "  subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "  logging.info(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  logging.basicConfig(level=logging.INFO)\n",
    "  fire.Fire(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 1.9 KiB before compression.\n",
      "Uploading tarball of [../training_image] to [gs://jk-demo1_cloudbuild/source/1566304857.53-947b99a4385c4006bfabc8c48e0fa840.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-demo1/builds/0a320474-6647-4f8b-9004-c8dd004be095].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/0a320474-6647-4f8b-9004-c8dd004be095?project=826865698127].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"0a320474-6647-4f8b-9004-c8dd004be095\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-demo1_cloudbuild/source/1566304857.53-947b99a4385c4006bfabc8c48e0fa840.tgz#1566304857965537\n",
      "Copying gs://jk-demo1_cloudbuild/source/1566304857.53-947b99a4385c4006bfabc8c48e0fa840.tgz#1566304857965537...\n",
      "/ [1 files][  1.0 KiB/  1.0 KiB]                                                \n",
      "Operation completed over 1 objects/1.0 KiB.                                      \n",
      "BUILD\n",
      "Pulling image: gcr.io/kaniko-project/executor:latest\n",
      "latest: Pulling from kaniko-project/executor\n",
      "Digest: sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n",
      "Status: Downloaded newer image for gcr.io/kaniko-project/executor:latest\n",
      "gcr.io/kaniko-project/executor:latest\n",
      "\u001b[36mINFO\u001b[0m[0000] Resolved base name gcr.io/jk-demo1/sklearn-cpu:latest to gcr.io/jk-demo1/sklearn-cpu:latest \n",
      "\u001b[36mINFO\u001b[0m[0000] Resolved base name gcr.io/jk-demo1/sklearn-cpu:latest to gcr.io/jk-demo1/sklearn-cpu:latest \n",
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image gcr.io/jk-demo1/sklearn-cpu:latest \n",
      "\u001b[36mINFO\u001b[0m[0000] Error while retrieving image from cache: getting file info: stat /cache/sha256:9641e43af0816270b5c797fd6009d6b65bd9a9775f014f3b58ea0e37d617bb3a: no such file or directory \n",
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image gcr.io/jk-demo1/sklearn-cpu:latest \n",
      "\u001b[36mINFO\u001b[0m[0000] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image gcr.io/jk-demo1/sklearn-cpu:latest \n",
      "\u001b[36mINFO\u001b[0m[0001] Error while retrieving image from cache: getting file info: stat /cache/sha256:9641e43af0816270b5c797fd6009d6b65bd9a9775f014f3b58ea0e37d617bb3a: no such file or directory \n",
      "\u001b[36mINFO\u001b[0m[0001] Downloading base image gcr.io/jk-demo1/sklearn-cpu:latest \n",
      "\u001b[36mINFO\u001b[0m[0001] Using files from context: [/workspace/train.py] \n",
      "\u001b[36mINFO\u001b[0m[0001] Skipping unpacking as no commands require it. \n",
      "\u001b[36mINFO\u001b[0m[0001] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0001] WORKDIR /app                                 \n",
      "\u001b[36mINFO\u001b[0m[0001] cmd: workdir                                 \n",
      "\u001b[36mINFO\u001b[0m[0001] Changed working directory to /app            \n",
      "\u001b[36mINFO\u001b[0m[0001] Creating directory /app                      \n",
      "\u001b[36mINFO\u001b[0m[0001] Taking snapshot of files...                  \n",
      "\u001b[36mINFO\u001b[0m[0001] Using files from context: [/workspace/train.py] \n",
      "\u001b[36mINFO\u001b[0m[0001] COPY train.py .                              \n",
      "\u001b[36mINFO\u001b[0m[0001] Taking snapshot of files...                  \n",
      "\u001b[36mINFO\u001b[0m[0001] ENTRYPOINT [\"python\", \"train.py\"]            \n",
      "\u001b[36mINFO\u001b[0m[0001] No files changed in this command, skipping snapshotting. \n",
      "2019/08/20 12:41:13 existing blob: sha256:27364346da9f15cf2ed310941d2e7bb73956350d7252be2af9c9abb4febc069a\n",
      "2019/08/20 12:41:13 existing blob: sha256:8e7f4eb46254f843037a1c65cbd7b0e2fceae689fe2db7b39c2f30a0c37f92fe\n",
      "2019/08/20 12:41:13 existing blob: sha256:a0fccb56e473519006c7c3567e4e85d50f0e833604541e531117cbb4515a601a\n",
      "2019/08/20 12:41:13 existing blob: sha256:f961afefb5b48bd0161367c89a4df03581118e14b1f4bed638fd876e7969c161\n",
      "2019/08/20 12:41:13 existing blob: sha256:be6f29e4b5b17aff2c0b5c73b71f537280752170aba53cfa05d2aaba28854b8f\n",
      "2019/08/20 12:41:13 existing blob: sha256:0fe7e7cbb2e88617d969efeeb3bd3125f7d309335c736a0525233ec2dc06aee1\n",
      "2019/08/20 12:41:13 existing blob: sha256:1d425c98234572d4221a1ac173162c4279f9fdde4726ec22ad3c399f59bb7503\n",
      "2019/08/20 12:41:13 existing blob: sha256:1c75398b6b38b33b19ef8cb52c5a2b290182227e6267555623e8d1f76813a74c\n",
      "2019/08/20 12:41:13 existing blob: sha256:376e851883a7316289c8dbd4da636de7cdd3756b5b653dcb30c7a2a9cd9a2c66\n",
      "2019/08/20 12:41:13 existing blob: sha256:63099198646c4864de393c33a7e013e0b7f369a0b5c4283bc9ed8b66ed62f13c\n",
      "2019/08/20 12:41:13 existing blob: sha256:32b22702030c512ccdfea8f820519d9ec017a85e36cadf8aff9ed323d2edb6d8\n",
      "2019/08/20 12:41:13 existing blob: sha256:a74bd22c075359ac48a9f161bc4e6901d4a5289903d7653af5fc3c8d9e58d6fd\n",
      "2019/08/20 12:41:13 existing blob: sha256:344da5c95cecd0f55238ce59b8469ee301056001ece2b769e9691b80f94f9f37\n",
      "2019/08/20 12:41:13 existing blob: sha256:7413c47ba209e555018c4be91101d017737f24b0c9d1f65339b97a4da98acb2a\n",
      "2019/08/20 12:41:13 existing blob: sha256:23aa97798c19c7b3722715e384d840803fde9cbdf59b1ea574ab3f184d2bae8c\n",
      "2019/08/20 12:41:13 existing blob: sha256:6ed4546c074e590fa0e18249a743b6ef4791a0357f1e65ff1875af3baa251505\n",
      "2019/08/20 12:41:13 existing blob: sha256:be36d213ecbafdf6a96ff10fbc2a53311cb116c7359a3d34a31703384e712e48\n",
      "2019/08/20 12:41:13 existing blob: sha256:bf6b8299d4b6a75ba222f5030c878756efc496ad3fa884b9ce8eac03ab9eb9b4\n",
      "2019/08/20 12:41:14 pushed blob: sha256:651055922f97738c9f74a82c0d8a13645a2b403bfb91b2885e50d514b2de55ab\n",
      "2019/08/20 12:41:14 pushed blob: sha256:8181e0f0f06e63b53a489a11c76806f189e7e067a48b21f220a531ccd121950a\n",
      "2019/08/20 12:41:14 pushed blob: sha256:2a0228e0bf431ded30f7b395dcf0e891c8bd8837fb6c89e994bb181df358f6e8\n",
      "2019/08/20 12:41:15 gcr.io/jk-demo1/octane-regression-training:latest: digest: sha256:2c4f45e378429edd2ba379046112ed0062e26005231659ccef5b47fbc845bb3e size: 3497\n",
      "PUSH\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                              IMAGES  STATUS\n",
      "0a320474-6647-4f8b-9004-c8dd004be095  2019-08-20T12:40:58+00:00  19S       gs://jk-demo1_cloudbuild/source/1566304857.53-947b99a4385c4006bfabc8c48e0fa840.tgz  -       SUCCESS\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = !gcloud config list project --format \"value(core.project)\"\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "IMAGE_REPO_NAME=\"octane-regression-training\"\n",
    "IMAGE_TAG=\"latest\"\n",
    "IMAGE_URI=\"gcr.io/{}/{}:{}\".format(PROJECT_ID, IMAGE_REPO_NAME, IMAGE_TAG)\n",
    "\n",
    "!gcloud builds submit --tag $IMAGE_URI $APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20190820_124126] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20190820_124126\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20190820_124126\n",
      "jobId: JOB_20190820_124126\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME=\"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "REGION=\"us-west1\"\n",
    "SCALE_TIER=\"BASIC\"\n",
    "JOB_DIR=\"gs://jk-demo-jobdir/{}\".format(JOB_NAME)\n",
    "\n",
    "DATA_PATH=\"gs://jk-demo-datasets/gasdata/training.csv\"\n",
    "N_FEATURES_OPTIONS=\"[2,4,6]\"\n",
    "L2_REG_OPTIONS=\"[0.1,0.2,0.3,0.5]\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region $REGION \\\n",
    "--job-dir $JOB_DIR \\\n",
    "--master-image-uri $IMAGE_URI \\\n",
    "--scale-tier $SCALE_TIER \\\n",
    "-- \\\n",
    "--data_path $DATA_PATH \\\n",
    "--n_features_options $N_FEATURES_OPTIONS \\\n",
    "--l2_reg_options $L2_REG_OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2019-08-20T12:41:27Z'\n",
      "etag: V3J2D1Nij90=\n",
      "jobId: JOB_20190820_124126\n",
      "state: PREPARING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --data_path\n",
      "  - gs://jk-demo-datasets/gasdata/training.csv\n",
      "  - --n_features_options\n",
      "  - '[2,4,6]'\n",
      "  - --l2_reg_options\n",
      "  - '[0.1,0.2,0.3,0.5]'\n",
      "  jobDir: gs://jk-demo-jobdir/JOB_20190820_124126\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/jk-demo1/octane-regression-training:latest\n",
      "  region: us-west1\n",
      "trainingOutput: {}\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20190820_124126?project=jk-demo1\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2FJOB_20190820_124126&project=jk-demo1\n",
      "INFO\t2019-08-20 12:41:26 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2019-08-20 12:41:27 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2019-08-20 12:41:27 +0000\tservice\t\tJob JOB_20190820_124126 is queued.\n",
      "INFO\t2019-08-20 12:41:27 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2019-08-20 12:41:30 +0000\tservice\t\tWaiting for training program to start.\n",
      "ERROR\t2019-08-20 12:45:20 +0000\tmaster-replica-0\t\tINFO:root:Best estimator: {'reduce_dim__n_components': 6, 'regress__alpha': 0.1}\n",
      "ERROR\t2019-08-20 12:45:20 +0000\tmaster-replica-0\t\tINFO:root:Best score: -0.04649019025311002\n",
      "INFO\t2019-08-20 12:45:21 +0000\tmaster-replica-0\t\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-08-20 12:45:21 +0000\tmaster-replica-0\t\t/ [0 files][    0.0 B/ 33.1 KiB]                                                \n",
      "INFO\t2019-08-20 12:45:21 +0000\tmaster-replica-0\t\t/ [1 files][ 33.1 KiB/ 33.1 KiB]                                                \n",
      "INFO\t2019-08-20 12:45:21 +0000\tmaster-replica-0\t\tOperation completed over 1 objects/33.1 KiB.                                     \n",
      "ERROR\t2019-08-20 12:45:22 +0000\tmaster-replica-0\t\tINFO:root:Saved model in: gs://jk-demo-jobdir/JOB_20190820_124126/trained_model/model.joblib\n",
      "INFO\t2019-08-20 12:48:21 +0000\tservice\t\tJob completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME\n",
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy using AI Platform Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom prediction routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_FOLDER = '../predict_app/'\n",
    "os.makedirs(APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Predictor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../predict_app//predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $APP_FOLDER/predict.py\n",
    "\n",
    "\n",
    "\n",
    "class OctaneRegressor(object):\n",
    "    \"\"\"A custom prediction routine for Octane regressor\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        \"\"\"Stores the model loaded in from_path\"\"\"\n",
    "        self._model = model\n",
    "        \n",
    "    def predict(self, instances, **kwargs):\n",
    "        \"\"\"Runs inference\"\"\"\n",
    "    \n",
    "        inputs = np.asarray(instances)\n",
    "        outputs = self._model.predict(preprocessed_inputs)\n",
    "        \n",
    "        return outputs.tolist()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "        \"\"\"Loads the model from the joblib file\"\"\"\n",
    "        #model_path = os.path.join(model_dir, 'model.joblib')\n",
    "        #model = joblib.load(model_path)\n",
    "        model = None\n",
    "        \n",
    "        return cls(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../predict_app//setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $APP_FOLDER/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['scikit-learn>=0.21.2', 'pandas>=0.25.0']\n",
    "\n",
    "setup(\n",
    "    name='custom-predictor',\n",
    "    description='Custom prediction routine.',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    scripts=['predict.py']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Python source distribution package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating custom_predictor.egg-info\n",
      "writing top-level names to custom_predictor.egg-info/top_level.txt\n",
      "writing requirements to custom_predictor.egg-info/requires.txt\n",
      "writing dependency_links to custom_predictor.egg-info/dependency_links.txt\n",
      "writing custom_predictor.egg-info/PKG-INFO\n",
      "writing manifest file 'custom_predictor.egg-info/SOURCES.txt'\n",
      "reading manifest file 'custom_predictor.egg-info/SOURCES.txt'\n",
      "writing manifest file 'custom_predictor.egg-info/SOURCES.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating custom-predictor-0.1\n",
      "creating custom-predictor-0.1/custom_predictor.egg-info\n",
      "copying files to custom-predictor-0.1...\n",
      "copying custom_predictor.egg-info/PKG-INFO -> custom-predictor-0.1/custom_predictor.egg-info\n",
      "copying custom_predictor.egg-info/SOURCES.txt -> custom-predictor-0.1/custom_predictor.egg-info\n",
      "copying custom_predictor.egg-info/dependency_links.txt -> custom-predictor-0.1/custom_predictor.egg-info\n",
      "copying custom_predictor.egg-info/requires.txt -> custom-predictor-0.1/custom_predictor.egg-info\n",
      "copying custom_predictor.egg-info/top_level.txt -> custom-predictor-0.1/custom_predictor.egg-info\n",
      "Writing custom-predictor-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'custom-predictor-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "%run ../predict_app/setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the source distribution package to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dist/CustomPredictor-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  554.0 B/  554.0 B]                                                \n",
      "Operation completed over 1 objects/554.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "DEPLOYMENT_VERSION = \"octane_v3\"\n",
    "CUSTOM_PREDICTOR_PATH = \"gs://jk-demo-staging/{}/{}\".format(DEPLOYMENT_VERSION, \"custom-predictor-0.1.tar.gz\")\n",
    "\n",
    "!gsutil cp dist/CustomPredictor-0.1.tar.gz $CUSTOM_PREDICTOR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.models.create) Resource in project [jk-demo1] is the subject of a conflict: Field: model.name Error: A model with the same name already exists.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: A model with the same name already exists.\n",
      "    field: model.name\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gas_octane_regression\"\n",
    "REGION = \"us-central1\"\n",
    "LABELS = \"task=regression,domain=chemometrics\"\n",
    "\n",
    "!gcloud ai-platform models create  $MODEL_NAME \\\n",
    "--regions=$REGION \\\n",
    "--labels=$LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......failed.                  \n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.ai-platform.versions.create) Create Version failed. Bad model detected with error:  \"Failed to load model: User-provided package custom-predictor-0.1.tar.gz failed to install: Command '['python-default', '-m', 'pip', 'install', '--target=/tmp/custom_lib', '--no-cache-dir', '-b', '/tmp/pip_builds', '/tmp/custom_code/custom-predictor-0.1.tar.gz']' returned non-zero exit status 1 (Error code: 0)\"\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"{}/trained_model/\".format(JOB_DIR)\n",
    "\n",
    "!gcloud beta ai-platform versions create $DEPLOYMENT_VERSION \\\n",
    "--model=$MODEL_NAME \\\n",
    "--origin=$MODEL_PATH \\\n",
    "--runtime-version=1.14 \\\n",
    "--python-version=3.5 \\\n",
    "--package-uris $CUSTOM_PREDICTOR_PATH \\\n",
    "--prediction-class predict.OctaneRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "#### Prepare a file with test instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATASET_PATH = \"gs://jk-demo-datasets/gasdata/testing.csv\"\n",
    "INPUT_FILE = 'instances.json'\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATASET_PATH).drop('octane', axis=1)\n",
    "with open(INPUT_FILE, \"w\") as f:\n",
    "  for index, row in df_test.iterrows():\n",
    "    f.write(json.dumps(list(row.values)))\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model $MODEL_NAME \\\n",
    "--version $VERSION_NAME \\\n",
    "--json-instances $INPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://jk-demo-jobdir/JOB_20190820_124126/trained_model/model.joblib...\n",
      "/ [1 files][ 33.1 KiB/ 33.1 KiB]                                                \n",
      "Operation completed over 1 objects/33.1 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "LOCAL_PATH = '/tmp/model.joblib'\n",
    "GCS_PATH = \"{}/trained_model/model.joblib\".format(JOB_DIR)\n",
    "\n",
    "!gsutil cp $GCS_PATH $LOCAL_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
