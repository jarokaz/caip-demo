{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U fire cloudml-hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fire\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "ARTIFACT_STORE = 'gs://hostedkfp-default-l2iv13wnek'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a training app for a toy ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_app_folder = 'training_app'\n",
    "os.makedirs(training_app_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {training_app_folder}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def _create_toy_resnet(dropout_rate=0.5, learning_rate=0.001):\n",
    "    inputs = keras.Input(shape=(32, 32, 3), name='img')\n",
    "    x = layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "    x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "    block_1_output = layers.MaxPooling2D(3)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(block_1_output)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    block_2_output = layers.add([x, block_1_output])\n",
    "\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(block_2_output)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    block_3_output = layers.add([x, block_2_output])\n",
    "\n",
    "    x = layers.Conv2D(64, 3, activation='relu')(block_3_output)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(10)(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name='toy_resnet')\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(learning_rate),\n",
    "              loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "class _HptuneCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    hpt = hypertune.HyperTune()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        _HptuneCallback.hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='accuracy',\n",
    "            metric_value=logs['val_acc'],\n",
    "            global_step=epoch\n",
    "    )\n",
    "        \n",
    "        \n",
    "def train_evaluate(job_dir, dropout_rate, learning_rate, batch_size, num_epochs):\n",
    "    \n",
    "    toy_resnet = _create_toy_resnet(dropout_rate, learning_rate)\n",
    "    toy_resnet.summary()\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    toy_resnet.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epochs,\n",
    "          validation_split=0.2,\n",
    "          verbose=2,\n",
    "          callbacks=[_HptuneCallback()])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01 05:02:56.618551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-01 05:02:59.103990: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "2020-04-01 05:02:59.105581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n",
      "2020-04-01 05:03:01.044874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-04-01 05:03:03.006699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-01 05:03:03.007715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2020-04-01 05:03:03.007813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-01 05:03:03.007902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-04-01 05:03:03.010486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-04-01 05:03:03.011046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-04-01 05:03:03.013602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-04-01 05:03:03.014818: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-04-01 05:03:03.014879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-04-01 05:03:03.015099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-01 05:03:03.015904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-01 05:03:03.016677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-04-01 05:03:03.026685: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
      "2020-04-01 05:03:03.026993: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b13dc34aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-04-01 05:03:03.027033: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-04-01 05:03:03.074470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-01 05:03:03.075372: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b13dcaaf40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-04-01 05:03:03.075415: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2020-04-01 05:03:03.075699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-01 05:03:03.076522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2020-04-01 05:03:03.076579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-01 05:03:03.076661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-04-01 05:03:03.076741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-04-01 05:03:03.076784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-04-01 05:03:03.076878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-04-01 05:03:03.076943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-04-01 05:03:03.076988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-04-01 05:03:03.077078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-01 05:03:03.077957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-01 05:03:03.078771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-04-01 05:03:03.078909: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-04-01 05:03:03.746301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-04-01 05:03:03.746369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
      "2020-04-01 05:03:03.746395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
      "2020-04-01 05:03:03.746826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-01 05:03:03.747815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-04-01 05:03:03.748741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10714 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Model: \"toy_resnet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img (InputLayer)                [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 30, 30, 32)   896         img[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 64)   18496       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 9, 9, 64)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 9, 9, 64)     36928       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 9, 9, 64)     36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 9, 9, 64)     0           conv2d_3[0][0]                   \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 9, 9, 64)     36928       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 9, 9, 64)     36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 9, 9, 64)     0           conv2d_5[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 7, 7, 64)     36928       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 64)           0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          16640       global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 223,242\n",
      "Trainable params: 223,242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "2020-04-01 05:03:07.234309: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-04-01 05:03:07.419678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "40000/40000 - 18s - loss: 1.8836 - acc: 0.2833 - val_loss: 1.6213 - val_acc: 0.3889\n",
      "Epoch 2/5\n",
      "40000/40000 - 15s - loss: 1.4797 - acc: 0.4586 - val_loss: 1.2277 - val_acc: 0.5542\n",
      "Epoch 3/5\n",
      "40000/40000 - 15s - loss: 1.2643 - acc: 0.5458 - val_loss: 1.3485 - val_acc: 0.5452\n",
      "Epoch 4/5\n",
      "40000/40000 - 15s - loss: 1.1081 - acc: 0.6079 - val_loss: 1.0409 - val_acc: 0.6313\n",
      "Epoch 5/5\n",
      "40000/40000 - 15s - loss: 0.9862 - acc: 0.6532 - val_loss: 1.0175 - val_acc: 0.6412\n"
     ]
    }
   ],
   "source": [
    "job_dir = '/home/jupyter/jobs/job1'\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "!python training_app/train.py {job_dir} {dropout_rate} {learning_rate} {batch_size} {num_epochs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package the training app into a docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {training_app_folder}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-1\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = 'toy_resent_trainer_image'\n",
    "image_tag = 'latest'\n",
    "image_uri = 'gcr.io/{}/{}:{}'.format(PROJECT_ID, image_name, image_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 4.2 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://mlops-workshop_cloudbuild/source/1585717466.3-670f0fae3e8a437b9a0a1f798ef38a0b.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-workshop/builds/e7c02d20-cf3f-44a5-b23d-57b8b6dc98b0].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/e7c02d20-cf3f-44a5-b23d-57b8b6dc98b0?project=745302968357].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"e7c02d20-cf3f-44a5-b23d-57b8b6dc98b0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-workshop_cloudbuild/source/1585717466.3-670f0fae3e8a437b9a0a1f798ef38a0b.tgz#1585717466671540\n",
      "Copying gs://mlops-workshop_cloudbuild/source/1585717466.3-670f0fae3e8a437b9a0a1f798ef38a0b.tgz#1585717466671540...\n",
      "/ [1 files][  1.7 KiB/  1.7 KiB]                                                \n",
      "Operation completed over 1 objects/1.7 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   7.68kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-1\n",
      "latest: Pulling from deeplearning-platform-release/tf2-gpu.2-1\n",
      "7ddbc47eeb70: Pulling fs layer\n",
      "c1bbdc448b72: Pulling fs layer\n",
      "8c3b70e39044: Pulling fs layer\n",
      "45d437916d57: Pulling fs layer\n",
      "d8f1569ddae6: Pulling fs layer\n",
      "85386706b020: Pulling fs layer\n",
      "ee9b457b77d0: Pulling fs layer\n",
      "be4f3343ecd3: Pulling fs layer\n",
      "30b4effda4fd: Pulling fs layer\n",
      "b398e882f414: Pulling fs layer\n",
      "68282a058bbe: Pulling fs layer\n",
      "79eeb7ceefad: Pulling fs layer\n",
      "750b29e9eb15: Pulling fs layer\n",
      "e4be17aa6db9: Pulling fs layer\n",
      "1ecb60916452: Pulling fs layer\n",
      "85b1460ab2dd: Pulling fs layer\n",
      "f840da7abf6f: Pulling fs layer\n",
      "7df56b5a1b90: Pulling fs layer\n",
      "a2724a388c5b: Pulling fs layer\n",
      "444a423384b3: Pulling fs layer\n",
      "5983ae55b557: Pulling fs layer\n",
      "2a832f6e7d9a: Pulling fs layer\n",
      "15956124bc4d: Pulling fs layer\n",
      "3d8c4f142ed1: Pulling fs layer\n",
      "45d437916d57: Waiting\n",
      "d8f1569ddae6: Waiting\n",
      "85386706b020: Waiting\n",
      "ee9b457b77d0: Waiting\n",
      "be4f3343ecd3: Waiting\n",
      "30b4effda4fd: Waiting\n",
      "b398e882f414: Waiting\n",
      "68282a058bbe: Waiting\n",
      "79eeb7ceefad: Waiting\n",
      "750b29e9eb15: Waiting\n",
      "e4be17aa6db9: Waiting\n",
      "1ecb60916452: Waiting\n",
      "85b1460ab2dd: Waiting\n",
      "f840da7abf6f: Waiting\n",
      "7df56b5a1b90: Waiting\n",
      "a2724a388c5b: Waiting\n",
      "444a423384b3: Waiting\n",
      "5983ae55b557: Waiting\n",
      "2a832f6e7d9a: Waiting\n",
      "15956124bc4d: Waiting\n",
      "3d8c4f142ed1: Waiting\n",
      "c1bbdc448b72: Verifying Checksum\n",
      "c1bbdc448b72: Download complete\n",
      "8c3b70e39044: Verifying Checksum\n",
      "8c3b70e39044: Download complete\n",
      "7ddbc47eeb70: Verifying Checksum\n",
      "7ddbc47eeb70: Download complete\n",
      "45d437916d57: Verifying Checksum\n",
      "45d437916d57: Download complete\n",
      "d8f1569ddae6: Verifying Checksum\n",
      "d8f1569ddae6: Download complete\n",
      "ee9b457b77d0: Verifying Checksum\n",
      "ee9b457b77d0: Download complete\n",
      "85386706b020: Verifying Checksum\n",
      "85386706b020: Download complete\n",
      "b398e882f414: Verifying Checksum\n",
      "b398e882f414: Download complete\n",
      "30b4effda4fd: Verifying Checksum\n",
      "30b4effda4fd: Download complete\n",
      "be4f3343ecd3: Verifying Checksum\n",
      "be4f3343ecd3: Download complete\n",
      "750b29e9eb15: Verifying Checksum\n",
      "750b29e9eb15: Download complete\n",
      "68282a058bbe: Verifying Checksum\n",
      "68282a058bbe: Download complete\n",
      "79eeb7ceefad: Verifying Checksum\n",
      "79eeb7ceefad: Download complete\n",
      "1ecb60916452: Verifying Checksum\n",
      "1ecb60916452: Download complete\n",
      "f840da7abf6f: Verifying Checksum\n",
      "f840da7abf6f: Download complete\n",
      "85b1460ab2dd: Verifying Checksum\n",
      "85b1460ab2dd: Download complete\n",
      "7df56b5a1b90: Verifying Checksum\n",
      "7df56b5a1b90: Download complete\n",
      "7ddbc47eeb70: Pull complete\n",
      "a2724a388c5b: Verifying Checksum\n",
      "a2724a388c5b: Download complete\n",
      "e4be17aa6db9: Verifying Checksum\n",
      "e4be17aa6db9: Download complete\n",
      "444a423384b3: Verifying Checksum\n",
      "444a423384b3: Download complete\n",
      "c1bbdc448b72: Pull complete\n",
      "8c3b70e39044: Pull complete\n",
      "15956124bc4d: Verifying Checksum\n",
      "15956124bc4d: Download complete\n",
      "45d437916d57: Pull complete\n",
      "3d8c4f142ed1: Verifying Checksum\n",
      "3d8c4f142ed1: Download complete\n",
      "5983ae55b557: Verifying Checksum\n",
      "5983ae55b557: Download complete\n",
      "d8f1569ddae6: Pull complete\n",
      "85386706b020: Pull complete\n",
      "ee9b457b77d0: Pull complete\n",
      "2a832f6e7d9a: Verifying Checksum\n",
      "2a832f6e7d9a: Download complete\n",
      "be4f3343ecd3: Pull complete\n",
      "30b4effda4fd: Pull complete\n",
      "b398e882f414: Pull complete\n",
      "68282a058bbe: Pull complete\n",
      "79eeb7ceefad: Pull complete\n",
      "750b29e9eb15: Pull complete\n",
      "e4be17aa6db9: Pull complete\n",
      "1ecb60916452: Pull complete\n",
      "85b1460ab2dd: Pull complete\n",
      "f840da7abf6f: Pull complete\n",
      "7df56b5a1b90: Pull complete\n",
      "a2724a388c5b: Pull complete\n",
      "444a423384b3: Pull complete\n",
      "5983ae55b557: Pull complete\n",
      "2a832f6e7d9a: Pull complete\n",
      "15956124bc4d: Pull complete\n",
      "3d8c4f142ed1: Pull complete\n",
      "Digest: sha256:fb3cd72baeb9c39e74d9bf7224cb36f93ac505494d771926b06cf4734b6e5a37\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-gpu.2-1:latest\n",
      " ---> 4c5f4efaf2dd\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in 2093cceb09c9\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.0.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor in /opt/conda/lib/python3.7/site-packages (from fire) (1.1.0)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.0-py2.py3-none-any.whl size=111108 sha256=27bd7861e495b0929da8af2649864d454da390ad224c4b18254192ae3db1bcb0\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/91/7b/2954fc66c99d47e663eac5d22a6b5a9bf39a230fa80e6782e8\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=10004635294dbaa4d53c4b386ccbd0c99c0234f488ee64b47e73bfc3fac724d1\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: fire, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.0\n",
      "Removing intermediate container 2093cceb09c9\n",
      " ---> 895afcf4f84e\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 9cdae493f8f9\n",
      "Removing intermediate container 9cdae493f8f9\n",
      " ---> 99c6daf4f3f2\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> b39f89585ec6\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 42b2469fe633\n",
      "Removing intermediate container 42b2469fe633\n",
      " ---> 8ac7621947a9\n",
      "Successfully built 8ac7621947a9\n",
      "Successfully tagged gcr.io/mlops-workshop/toy_resent_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-workshop/toy_resent_trainer_image:latest\n",
      "The push refers to repository [gcr.io/mlops-workshop/toy_resent_trainer_image]\n",
      "951363298c88: Preparing\n",
      "38d4f92e843f: Preparing\n",
      "ae0c16badf9d: Preparing\n",
      "b06c04f18f4e: Preparing\n",
      "4c4517a7db12: Preparing\n",
      "369f574177f9: Preparing\n",
      "42c00288ef8a: Preparing\n",
      "992939e921d8: Preparing\n",
      "a304fb96c494: Preparing\n",
      "431f13f6088f: Preparing\n",
      "f63b09c90bb4: Preparing\n",
      "67bbf9d41400: Preparing\n",
      "c77f21bf84a6: Preparing\n",
      "850faaeb3b68: Preparing\n",
      "428e6ac999c3: Preparing\n",
      "c37afc73f3e6: Preparing\n",
      "3d1e6282f4c6: Preparing\n",
      "1db09913a256: Preparing\n",
      "2e282f599fd6: Preparing\n",
      "e6f174f76be4: Preparing\n",
      "808fd332a58a: Preparing\n",
      "b16af11cbf29: Preparing\n",
      "37b9a4b22186: Preparing\n",
      "e0b3afb09dc3: Preparing\n",
      "6c01b5a53aac: Preparing\n",
      "2c6ac8e5063e: Preparing\n",
      "cc967c529ced: Preparing\n",
      "369f574177f9: Waiting\n",
      "42c00288ef8a: Waiting\n",
      "992939e921d8: Waiting\n",
      "a304fb96c494: Waiting\n",
      "431f13f6088f: Waiting\n",
      "f63b09c90bb4: Waiting\n",
      "67bbf9d41400: Waiting\n",
      "c77f21bf84a6: Waiting\n",
      "850faaeb3b68: Waiting\n",
      "428e6ac999c3: Waiting\n",
      "c37afc73f3e6: Waiting\n",
      "3d1e6282f4c6: Waiting\n",
      "1db09913a256: Waiting\n",
      "2e282f599fd6: Waiting\n",
      "e6f174f76be4: Waiting\n",
      "808fd332a58a: Waiting\n",
      "b16af11cbf29: Waiting\n",
      "37b9a4b22186: Waiting\n",
      "e0b3afb09dc3: Waiting\n",
      "6c01b5a53aac: Waiting\n",
      "2c6ac8e5063e: Waiting\n",
      "cc967c529ced: Waiting\n",
      "b06c04f18f4e: Layer already exists\n",
      "4c4517a7db12: Layer already exists\n",
      "369f574177f9: Layer already exists\n",
      "42c00288ef8a: Layer already exists\n",
      "992939e921d8: Layer already exists\n",
      "a304fb96c494: Layer already exists\n",
      "431f13f6088f: Layer already exists\n",
      "f63b09c90bb4: Layer already exists\n",
      "67bbf9d41400: Layer already exists\n",
      "c77f21bf84a6: Layer already exists\n",
      "850faaeb3b68: Layer already exists\n",
      "428e6ac999c3: Layer already exists\n",
      "c37afc73f3e6: Layer already exists\n",
      "3d1e6282f4c6: Layer already exists\n",
      "1db09913a256: Layer already exists\n",
      "2e282f599fd6: Layer already exists\n",
      "e6f174f76be4: Layer already exists\n",
      "808fd332a58a: Layer already exists\n",
      "37b9a4b22186: Layer already exists\n",
      "b16af11cbf29: Layer already exists\n",
      "e0b3afb09dc3: Layer already exists\n",
      "6c01b5a53aac: Layer already exists\n",
      "2c6ac8e5063e: Layer already exists\n",
      "cc967c529ced: Layer already exists\n",
      "ae0c16badf9d: Pushed\n",
      "38d4f92e843f: Pushed\n",
      "951363298c88: Pushed\n",
      "latest: digest: sha256:6a5646eaa0594ccb2a9e090b7ef8ffaaab84d29b4638665f97282841e43fb898 size: 5983\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                   IMAGES                                                    STATUS\n",
      "e7c02d20-cf3f-44a5-b23d-57b8b6dc98b0  2020-04-01T05:04:27+00:00  4M44S     gs://mlops-workshop_cloudbuild/source/1585717466.3-670f0fae3e8a437b9a0a1f798ef38a0b.tgz  gcr.io/mlops-workshop/toy_resent_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $image_uri $training_app_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit AI Platform Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "job_dir = '{}/{}'.format(ARTIFACT_STORE, job_name)\n",
    "scale_tier = 'BASIC_GPU'\n",
    "region = 'us-central1'\n",
    "\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200401_051015] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200401_051015\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200401_051015\n",
      "jobId: JOB_20200401_051015\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training {job_name} \\\n",
    "--region={region} \\\n",
    "--job-dir={job_dir} \\\n",
    "--master-image-uri={image_uri} \\\n",
    "--scale-tier={scale_tier} \\\n",
    "-- \\\n",
    "--dropout_rate={dropout_rate} \\\n",
    "--learning_rate={learning_rate} \\\n",
    "--batch_size={batch_size} \\\n",
    "--num_epochs={num_epochs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-04-01T05:10:18Z'\n",
      "endTime: '2020-04-01T05:19:06Z'\n",
      "etag: hL9SukdNePQ=\n",
      "jobId: JOB_20200401_051015\n",
      "startTime: '2020-04-01T05:15:01Z'\n",
      "state: SUCCEEDED\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --dropout_rate=0.5\n",
      "  - --learning_rate=0.001\n",
      "  - --batch_size=64\n",
      "  - --num_epochs=5\n",
      "  jobDir: gs://hostedkfp-default-l2iv13wnek/JOB_20200401_051015\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/mlops-workshop/toy_resent_trainer_image:latest\n",
      "  region: us-central1\n",
      "  scaleTier: BASIC_GPU\n",
      "trainingOutput:\n",
      "  consumedMLUnits: 0.28\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20200401_051015?project=mlops-workshop\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2FJOB_20200401_051015&project=mlops-workshop\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe {job_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2020-04-01 05:10:17 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2020-04-01 05:10:18 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2020-04-01 05:10:18 +0000\tservice\t\tJob JOB_20200401_051015 is queued.\n",
      "INFO\t2020-04-01 05:10:18 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2020-04-01 05:10:20 +0000\tservice\t\tWaiting for training program to start.\n",
      "ERROR\t2020-04-01 05:14:41 +0000\tmaster-replica-0\t\t2020-04-01 05:14:41.771117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "ERROR\t2020-04-01 05:14:43 +0000\tmaster-replica-0\t\t2020-04-01 05:14:43.146988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "ERROR\t2020-04-01 05:14:43 +0000\tmaster-replica-0\t\t2020-04-01 05:14:43.149322: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.097102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.140137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.141173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\tpciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\tcoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.141222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.141278: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.143735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.144246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.147153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.148902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.149116: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.149340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.150552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.151458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.161120: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.162040: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5576d05bbc70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.162074: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.232941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.234242: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5576cf2cffb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.234304: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.234923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.236029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\tpciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\tcoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.236086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.236122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.236154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.236177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.236197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.236217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.236258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.236366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.237326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.238190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "ERROR\t2020-04-01 05:14:44 +0000\tmaster-replica-0\t\t2020-04-01 05:14:44.238252: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "ERROR\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t2020-04-01 05:14:45.009436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "ERROR\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t2020-04-01 05:14:45.009510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
      "ERROR\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t2020-04-01 05:14:45.009525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
      "ERROR\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t2020-04-01 05:14:45.009854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "ERROR\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t2020-04-01 05:14:45.010893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "ERROR\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t2020-04-01 05:14:45.011743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10714 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tModel: \"toy_resnet\"\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t==================================================================================================\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\timg (InputLayer)                [(None, 32, 32, 3)]  0                                            \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tconv2d (Conv2D)                 (None, 30, 30, 32)   896         img[0][0]                        \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tconv2d_1 (Conv2D)               (None, 28, 28, 64)   18496       conv2d[0][0]                     \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tmax_pooling2d (MaxPooling2D)    (None, 9, 9, 64)     0           conv2d_1[0][0]                   \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tconv2d_2 (Conv2D)               (None, 9, 9, 64)     36928       max_pooling2d[0][0]              \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tconv2d_3 (Conv2D)               (None, 9, 9, 64)     36928       conv2d_2[0][0]                   \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tadd (Add)                       (None, 9, 9, 64)     0           conv2d_3[0][0]                   \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t                                                                 max_pooling2d[0][0]              \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tconv2d_4 (Conv2D)               (None, 9, 9, 64)     36928       add[0][0]                        \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tconv2d_5 (Conv2D)               (None, 9, 9, 64)     36928       conv2d_4[0][0]                   \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tadd_1 (Add)                     (None, 9, 9, 64)     0           conv2d_5[0][0]                   \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t                                                                 add[0][0]                        \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tconv2d_6 (Conv2D)               (None, 7, 7, 64)     36928       add_1[0][0]                      \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tglobal_average_pooling2d (Globa (None, 64)           0           conv2d_6[0][0]                   \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tdense (Dense)                   (None, 256)          16640       global_average_pooling2d[0][0]   \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tdropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tdense_1 (Dense)                 (None, 10)           2570        dropout[0][0]                    \n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t==================================================================================================\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tTotal params: 223,242\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tTrainable params: 223,242\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tNon-trainable params: 0\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2020-04-01 05:14:45 +0000\tmaster-replica-0\t\tDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t     8192/170498071 [..............................] - ETA: 7:\n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t   188416/170498071 [..............................] - ETA: 1:\n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t  1024000/170498071 [..............................] - ETA: 23s\n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t  4136960/170498071 [..............................] - ETA: 7\n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t  9592832/170498071 [>.............................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 15228928/170498071 [=>............................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 20414464/170498071 [==>...........................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 25714688/170498071 [===>..........................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 31236096/170498071 [====>.........................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 36757504/170498071 [=====>........................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 42106880/170498071 [======>.......................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 47104000/170498071 [=======>......................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 52486144/170498071 [========>.....................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 58056704/170498071 [=========>....................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 63389696/170498071 [==========>...................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 68296704/170498071 [===========>..................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 73474048/170498071 [===========>..................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 79077376/170498071 [============>.................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 84377600/170498071 [=============>................] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 89563136/170498071 [==============>...............] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t 94486528/170498071 [===============>..............] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t100024320/170498071 [================>.............] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t105439232/170498071 [=================>............] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t110780416/170498071 [==================>...........] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t115924992/170498071 [===================>..........] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t121069568/170498071 [====================>.........] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t126574592/170498071 [=====================>........] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t132046848/170498071 [======================>.......] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t137191424/170498071 [=======================>......] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t142237696/170498071 [========================>.....] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t147677184/170498071 [========================>.....] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t153214976/170498071 [=========================>....] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t158605312/170498071 [==========================>...] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t163676160/170498071 [===========================>..] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t169058304/170498071 [============================>.] - ETA: \n",
      "INFO\t2020-04-01 05:14:47 +0000\tmaster-replica-0\t\t170500096/170498071 [==============================] - 2s 0us/step\n",
      "ERROR\t2020-04-01 05:14:51 +0000\tmaster-replica-0\t\t2020-04-01 05:14:51.870126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "ERROR\t2020-04-01 05:14:52 +0000\tmaster-replica-0\t\t2020-04-01 05:14:52.074188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "INFO\t2020-04-01 05:15:08 +0000\tmaster-replica-0\t\tTrain on 40000 samples, validate on 10000 samples\n",
      "INFO\t2020-04-01 05:15:08 +0000\tmaster-replica-0\t\tEpoch 1/5\n",
      "INFO\t2020-04-01 05:15:08 +0000\tmaster-replica-0\t\t40000/40000 - 17s - loss: 1.9146 - acc: 0.2756 - val_loss: 1.6071 - val_acc: 0.3985\n",
      "INFO\t2020-04-01 05:15:22 +0000\tmaster-replica-0\t\tEpoch 2/5\n",
      "INFO\t2020-04-01 05:15:22 +0000\tmaster-replica-0\t\t40000/40000 - 15s - loss: 1.4860 - acc: 0.4548 - val_loss: 1.2156 - val_acc: 0.5631\n",
      "INFO\t2020-04-01 05:15:37 +0000\tmaster-replica-0\t\tEpoch 3/5\n",
      "INFO\t2020-04-01 05:15:37 +0000\tmaster-replica-0\t\t40000/40000 - 15s - loss: 1.2581 - acc: 0.5495 - val_loss: 1.0933 - val_acc: 0.6077\n",
      "INFO\t2020-04-01 05:15:52 +0000\tmaster-replica-0\t\tEpoch 4/5\n",
      "INFO\t2020-04-01 05:15:52 +0000\tmaster-replica-0\t\t40000/40000 - 15s - loss: 1.1121 - acc: 0.6082 - val_loss: 1.1280 - val_acc: 0.6000\n",
      "INFO\t2020-04-01 05:16:06 +0000\tmaster-replica-0\t\tEpoch 5/5\n",
      "INFO\t2020-04-01 05:16:06 +0000\tmaster-replica-0\t\t40000/40000 - 15s - loss: 0.9928 - acc: 0.6508 - val_loss: 0.9163 - val_acc: 0.6804\n",
      "INFO\t2020-04-01 05:19:06 +0000\tservice\t\tJob completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs {job_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {training_app_folder}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 10\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          32,\n",
    "          64\n",
    "          ]\n",
    "    - parameterName: dropout_rate\n",
    "      type: DOUBLE\n",
    "      minValue:  0.4\n",
    "      maxValue:  0.6\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue:  0.0005\n",
    "      maxValue:  0.002\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "job_dir = '{}/{}'.format(ARTIFACT_STORE, job_name)\n",
    "scale_tier = 'BASIC_GPU'\n",
    "region = 'us-central1'\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200401_052221] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200401_052221\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200401_052221\n",
      "jobId: JOB_20200401_052221\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training {job_name} \\\n",
    "--region={region} \\\n",
    "--job-dir={job_dir} \\\n",
    "--master-image-uri={image_uri} \\\n",
    "--scale-tier={scale_tier} \\\n",
    "--config {training_app_folder}/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--num_epochs={num_epochs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
